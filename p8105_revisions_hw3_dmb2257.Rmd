---
title: "p8105_revisions_hw3_dmb2257"
author: "Diane Benites"
date: "2025-11-21"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1

```{r}
#This loads necessary packages and the dataset. 

library(p8105.datasets)
library(tidyverse)
data("instacart")
```
Description of the Instacart Dataset: There are 1384617 observations of 15 variables. Each column represents one of the variables in the dataset. Each row represents one product of an order. This data provides information on when instacart orders in the sample were placed, which products were ordered, the frequency of orders, and the aisles associated with the orders. 


```{r}
items_ordered_df =
instacart|>
  janitor::clean_names()|>
  group_by(aisle_id, aisle)|>
  summarize(
    item_order_count=n())|>
  arrange(desc(item_order_count))
  
```
There are 134 aisles. The most items are ordered from aisle 83: fresh vegetables (150609 items ordered), aisle 24: fresh fruits (150473 items ordered), and aisle 123: packaged vegatables (78493 items ordered).


```{r}
# This makes a plot of the number of items ordered. The aisles are sorted by the number of items ordered. The names of the aisles are included on the x axis. 
items_ordered_df|>
  filter(item_order_count >= 10000)|>
  ggplot(
    aes(fct_reorder(aisle, item_order_count), item_order_count))+
  geom_col()+
  labs(
    x = "Aisle Name",
    y = "Number of Items Ordered"
  )+
  theme(axis.text.x = element_text(angle = 70, hjust =1))
  
```


```{r}
#This creates a table of the 3 most popular items in the aisles "baking ingredients", "dog food care", "packaged vegetables fruits", and how many of each item was ordered.

instacart|>
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits"))|>
  group_by(aisle, product_name)|>
  summarize(
    product_count = n()
  )|>
  arrange(desc(product_count))|>
  filter(min_rank(desc(product_count))< 4)|>
  knitr::kable()
```


```{r}
#This makes a table showing the mean hour of the day at which pank lady apples and coffee ice cream were ordered.

instacart |>
  mutate(
    order_dow = 
      case_match(
        order_dow,
        0 ~ "Sunday",
        1 ~ "Monday",
        2 ~ "Tuesday",
        3 ~ "Wednesday",
        4 ~ "Thursday",
        5 ~ "Friday",
        6 ~ "Saturday"),
    order_dow = as.factor(order_dow))|>
  group_by(product_name, order_dow)|>
  summarize(
    mean_hour = mean(order_hour_of_day, na.rm = TRUE)
  )|>
  filter(
    product_name %in% c("Pink Lady Apples", "Coffee Ice Cream"))|>
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour)|>
  knitr::kable()
```

# Problem 2

```{r}
#This imports the datasets. It cleans the second dataset to format the date and zip code variables. It also removed missing price data points. 

data_path <- 
"C:/Users/dmben/OneDrive/Desktop/data_science_1/p8105_revisions_dmb2257"


zipcodes_tidy_df = 
    read_csv(file.path(data_path, "Zip Codes.csv"))|>
  janitor::clean_names()

zori_df = 
  read_csv(file.path(data_path, "Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv"))|>
  pivot_longer(
    -(RegionID:CountyName),
    names_to = "dates",
    values_to = "price"
  )|>
  janitor::clean_names()|>
  rename(zip_code = region_name) |>
  mutate(
    dates = as_date(dates),
    zip_code = as.numeric(zip_code)
  )|>
  drop_na(price)

```


```{r}
# This code chunk summarizes the number of months each zip code was observed. 
zip_count=
zori_df|>
    mutate(
    month = floor_date(dates, unit = "month")
  )|>
  select(-dates)|>
  group_by(zip_code)|>
  summarize(
    zip_obs = n_distinct(month)
  )

zip_count|>
  filter(zip_obs == 116)

zip_count |>
  filter(zip_obs < 10)

```
There were 48 zip codes observed 116 times. There were 26 zip codes observed fewer than 10 times.

```{r}
average_price_df =
zori_df|>
  mutate(
    year = floor_date(dates, unit = "year"))|>
  rename(
    borough = county_name)|>
  separate(year, into=c("year", "month", "day"))|>
  select(-day, -dates)

#This creates a table of average rental price each year by borough

average_price_df|>
  group_by(borough, year)|>
  summarize(
    avg_price = mean(price)
  )|>
  pivot_wider(
    names_from = borough,
    values_from = avg_price
  )|>
  knitr::kable()
```
Among all boroughs, the average rental price increases each year, with the exception of an decrease in average price in 2020 in Kings, NY County and Queens and in 2021 in Kings and Queens. These differences may be attributed to changes in rent the covid-19 pandemic.

```{r}
average_price_df|>
  group_by(zip_code, borough, year)|>
  summarize(average_price = mean(price))|>
  ggplot(aes(x= year, y = average_price, color = borough)) +
  geom_point()+
  labs(title = "Average Rental Prices Within Zip Codes by Year",
       x = "Year",
       y = "Average Price")+
    theme(axis.text.x = element_text(angle = 70, hjust =1))

ggsave("problem2_plot1.jpg", plot = last_plot(), path = "results_hw2")
```


```{r}
zori_df|>
  mutate(
    year = floor_date(dates, unit = "month"))|>
  rename(
    borough = county_name)|>
  separate(year, into=c("year", "month", "day"))|>
  select(-day, -dates)|>
  filter(year == "2023")|>
  group_by(zip_code, borough, month)|>
  summarize(average_price = mean(price))|>
  ggplot(aes(x= month, y = average_price, color = borough)) +
  geom_point()+
  labs(title = "Average Rental Price Within Zip Codes by Months in 2023",
       x = "Month",
       y = "Average Price")

ggsave("problem2_plot2.jpg", plot = last_plot(), path = "results_hw2")
```


# Problem 3

```{r}
#The loads the accelerometer dataset and mutates the seqn variable to be a character.
accel_df = 
  read_csv(file = "./nhanes_accel (1).csv")|>
  janitor::clean_names()|>
  drop_na()|>
  mutate(seqn = as.character(seqn))

# This loads the demographic dataset. It omits the rows without values and sets the appropriate labels for the sex and education variables. 

covar_df = 
  read_csv(file = "./nhanes_covar.csv", col_names = c("SEQN", "sex", "age", "bmi", "education"))|>
  drop_na(SEQN)|>
  filter(!(SEQN == "SEQN"))|>
  janitor::clean_names()|>
  mutate(
    sex =
      case_match(
        sex,
        "1" ~ "male",
        "2" ~ "female"),
    education =
      case_match(
        education,
        "1" ~ "less than high school",
        "2" ~ "high school equivalent",
        "3" ~ "more than high school"),
    education = as.ordered(education)
    )

# This then joins both datasets and filters the merged dataframe to omit participants younger than 21 years old and with missing demographic information. 
merged_df = 
  left_join(accel_df, covar_df, by = "seqn")|>
  filter(age>=21)|>
  drop_na(sex, age, bmi, education)
```


```{r}
#This produces a reader friendly table of men and women in each education category. 
merged_df|>
  group_by(sex, education)|>
  summarize(n = n())|>
  pivot_wider(names_from = sex, values_from = n)|>
  knitr::kable()

#This plots the age distributions for men and women in each education category. 
sex_age=
merged_df|>
select(seqn, sex, age, education)|>
mutate(age = as.numeric(age))

sex_age|>
  group_by(seqn)|>
  ggplot()+
  geom_boxplot(aes(sex, age, fill = education))

```

The greatest number of men and women occur in the more than high school education level. There is a greater frequency of females in this category compared to men. The lowest frequency of females are in the high school equivalent category. The lowest frequency of males are in the less than high school category. 

The greatest median age of females occurs for those with a high school equivalent education level. The greatest median age of males occurs for those with less than high school education level. The median age of females with a high school equivalent is greater than the median age of males with a high school equivalent. However for  more than high school education, the median age for males is greater than the median age of females.



```{r}
# This aggregates the minutes for each participant.
aggregate_df=
merged_df|>
  pivot_longer(
    min1:min1440,
    names_to = "minutes",
    values_to = "activity"
  )|>
group_by(seqn)|>
  summarize(total_activity = sum(activity))

# This then plots the total activities against age, with pink for females and blue for males, and 3 separate panels for education. There is also a trend line to illustrate differences. 
plot_df = 
  left_join(merged_df, aggregate_df, by = "seqn")|>
  select(seqn, age, sex, education, total_activity)|>
  mutate(age = as.numeric(age))

plot_df|>  
  ggplot(aes(x = age, y = total_activity, color = sex))+
  geom_point()+
  geom_smooth(se = FALSE)+
  facet_grid(~education)

```
The steepest smooth curve occurs among those with less than high school education. Based on the smooth curve, the total activity generally decreases as age increases among males and females with less than high school education. Based on the smooth curves for high school equivalent and more than high school, females have greater total activity than males. 


```{r}
accel_plot_df = 
  merged_df|>
  pivot_longer(
    min1:min1440,
    names_to = "minutes",
    values_to = "activity"
  )|>
  mutate(minutes = str_remove(minutes, "min"))

#This code chunk plots accelerometer data by the 24 hour activity time courses for each education level, and uses color to indicate sex.

accel_plot_df|>
  group_by(seqn)|>
  mutate(minutes = as.numeric(minutes))|>
  ggplot(aes(x =minutes, y = activity))+
  geom_line(aes(color = sex))+
  geom_smooth(se = FALSE)+
  facet_grid(~education)+
  theme(axis.text.x = element_text(angle = 60, hjust =1))

```
Based on this graph, the greatest activity occurs for females among those with more than high school between minutes 1000 and 1500 of the day. The greatest activity for males occurs among those with more than high school between minutes 250 and 750 during the day. Also, the peak activity occurs among those with more than a high school education. The trend line shows similar patterns in activity throughout the day for all education levels, the lowest activity occurs during the beginning and end minutes of the day. 